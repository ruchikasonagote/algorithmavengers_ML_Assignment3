# -*- coding: utf-8 -*-
"""shakespearee.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FQ4l03tBLjhbrBm3WdXmKj8A-dtAtLDD
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import re
import torch.nn.functional as F
from torch import nn
import pandas as pd
import matplotlib.pyplot as plt # for making figures
from sklearn.manifold import TSNE
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
from pprint import pprint

torch.__version__

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

device

# Get some names from https://github.com/MASTREX/List-of-Indian-Names

# Open the file in read mode
with open('shakespeare.txt', 'r') as file:
    # Read the entire train of the file
    text = file.read()

text = text.lower()
text = text.replace("\n"," ")
text = re.sub(r'[^a-z\s]', '', text)
print(text)
train = text[:-3000]
test = text[-3000:]

len(train), len(test)

train

print(type(train))

# Create a dictionary to store unique characters and their indices
stoi = {}

stoi[' '] = 0

# Iterate through each character in the string
i = 1
for char in sorted(train):
    # Check for duplicate character
    if char not in stoi:
        # Adding the char to dictionary
        stoi[char] = i
        i += 1

# Print the dictionary
# print(stoi)

itos = {value: key for key, value in stoi.items()}

# print(itos)

block_size = 30
X, Y = [], []
for i in range(len(train)-block_size-2):
  X.append([stoi[x] for x in train[i:i+block_size]])
  Y.append(stoi[train[i+block_size]])

# Move data to GPU

X = torch.tensor(X).to(device)
Y = torch.tensor(Y).to(device)

# X.shape, X.dtype, Y.shape, Y.dtype

# Embedding layer
emb_dim = 25
emb = torch.nn.Embedding(len(stoi), emb_dim)

# emb.weight

# emb.weight.shape

class NextChar(nn.Module):
  def __init__(self, block_size, vocab_size, emb_dim, hidden_size1, hidden_size2):
    super().__init__()
    self.emb = nn.Embedding(vocab_size, emb_dim)
    self.lin1 = nn.Linear(block_size * emb_dim, hidden_size1)
    self.lin2 = nn.Linear(hidden_size1, hidden_size2)
    self.lin3 = nn.Linear(hidden_size2, vocab_size)

  def forward(self, x):
    x = self.emb(x)
    x = x.view(x.shape[0], -1)
    x = torch.sin(self.lin1(x)) # Activation function : change this
    x = self.lin2(x)
    return x

# Generate names from untrained model
model = NextChar(block_size, len(stoi), emb_dim, 500, 300).to(device)
# model = torch.compile(model)

no_of_chars = 200
g = torch.Generator()
g.manual_seed(200)

def generate_name(model, input_str, itos, stoi, block_size, max_len=no_of_chars):

    context = [0] * block_size
    # inp = inp.lower()
    if len(input_str) <= block_size:
      for i in range(len(input_str)):
        context[i] = stoi[input_str[i]]
    else:
      j = 0
      for i in range(len(input_str)-block_size,len(input_str)):
        context[j] = stoi[input_str[i]]
        j+=1

    name = ''
    for i in range(max_len):
        x = torch.tensor(context).view(1, -1).to(device)
        y_pred = model(x)
        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()
        if ix in itos:
          ch = itos[ix]
        # if ch == '.':
        #     break
          name += ch
          context = context[1:] + [ix]
    return name

print(generate_name(model, " ", itos, stoi, block_size, no_of_chars))

for param_name, param in model.named_parameters():
    print(param_name, param.shape)

# Train the model
loss_fn = nn.CrossEntropyLoss()
opt = torch.optim.AdamW(model.parameters(), lr=0.01)
import time
# Mini-batch training
batch_size = 4096
print_every = 100
elapsed_time = []
loss_arr = []
for epoch in range(200):
    start_time = time.time()
    for i in range(0, X.shape[0], batch_size):
        x = X[i:i+batch_size]
        y = Y[i:i+batch_size]
        y_pred = model(x)
        loss = loss_fn(y_pred, y)
        loss.backward()
        opt.step()
        opt.zero_grad()
    end_time = time.time()
    elapsed_time.append(end_time - start_time)
    loss_arr.append(loss.item())
    if epoch % print_every == 0:
        print(epoch, loss.item())

# Generate names from trained model
print(generate_name(model, "", itos, stoi, block_size, 10))

import numpy as np
# plt.plot(np.arange(200),loss_arr)
# plt.title("Loss vs Iterations. Emb size = 5. Block size = 30. 2 hidden layers")

torch.save(model.state_dict(),"gt_eng_model_upper_two_hid_layer_emb5_block_size_30.pth")

"""Tuning knobs

1. Embedding size
2. MLP
3. Context length
"""

if model.emb.weight.shape[1] > 2:
    embeddings = model.emb.weight.detach().cpu().numpy()
    perplexity = min(30, len(embeddings) - 1)  # Set perplexity to a maximum of 30 or one less than the number of samples
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)
    embeddings_2d = tsne.fit_transform(embeddings)
    plt.figure(figsize=(10, 8))
    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
    for i in range(len(itos)):
        plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1], itos[i])
    plt.title('t-SNE visualization of Embeddings')
    plt.show()
else:
    embeddings = model.emb.weight.detach().cpu().numpy()
    if embeddings.ndim == 1:
        plt.figure(figsize=(10, 8))
        for i, emb in enumerate(embeddings):
            plt.scatter(i, emb, color='k')
            plt.text(i + 0.05, emb + 0.05, itos[i])
        plt.title('1D Visualization of Embeddings')
        plt.show()
    else:
        plt.figure(figsize=(10, 8))
        for i in range(len(itos)):
            emb = embeddings[i][0]  # Extracting the single value from the array
            plt.scatter(i, emb, color='k')
            plt.text(i + 0.05, emb + 0.05, itos[i])
        plt.title('2D Visualization of Embeddings')
        plt.show()

